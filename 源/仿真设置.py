from pathlib import Path

from stable_baselines import PPO2

from 对比方法 import 预测最佳丢弃比例模型, 全丢模型, 决策树回归模型

测试序号 = 6

算法列表 = [
    # 'PPO+ARIMA',
    'ARIMA',
    # '全丢',
    # 'ARIMA+决策树回归'
]
算法对应模型 = {
    'PPO+ARIMA': PPO2,
    'ARIMA': 预测最佳丢弃比例模型,
    '全丢': 全丢模型,
    'ARIMA+决策树回归': 决策树回归模型
}

变量列表 = [
    '正常流量倍数',
    '攻击流量倍数',
    '正常流量检测准确度',
    '攻击流量检测准确度',
    '调整周期'
]
变量值表 = {
    '正常流量倍数': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    '攻击流量倍数': [0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5],
    '正常流量检测准确度': [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1],
    '攻击流量检测准确度': [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1],
    '调整周期': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
}
标准变量设置 = {
    '正常流量倍数': 6,
    '攻击流量倍数': 1,
    '正常流量检测准确度': 0.9,
    '攻击流量检测准确度': 0.9,
    '调整周期': 1
}

并行环境数 = 16
训练步数 = 10000000

仿真配置表 = {}
for 算法 in 算法列表:
    仿真配置表[算法] = {}
    for 变量 in 变量列表:
        仿真配置表[算法][变量] = {}
        for 变量值 in 变量值表[变量]:
            仿真配置表[算法][变量][变量值] = {
                '并行环境数': 并行环境数,
                '环境参数': {变量: 变量值},
                '模型设置': {
                    '模型': 算法对应模型[算法],
                    '模型参数': {
                        'policy': 'MlpPolicy',
                        'gamma': 0,
                        'n_steps': 500,
                        'nminibatches': 16,
                        'verbose': 1,
                        'tensorboard_log': Path(__file__).parent.parent / f'数据/测试{测试序号}/tensorboard/'
                    },
                    '学习参数': {
                        'total_timesteps': 训练步数,
                        'tb_log_name': f'{变量}{变量值}{算法}'
                    }
                },
                '保存设置': {
                    '模型保存位置': Path(__file__).parent.parent / f'数据/测试{测试序号}/模型/{变量}{变量值}{算法}',
                    '测试记录保存位置': Path(__file__).parent.parent / f'数据/测试{测试序号}/测试记录/{变量}{变量值}{算法}'
                }
            }

仿真配置列表 = []
for 变量 in 变量列表:
    for 变量值 in 变量值表[变量]:
        for 算法 in 算法列表:
            仿真配置列表.append(仿真配置表[算法][变量][变量值])
